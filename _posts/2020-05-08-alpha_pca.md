---
title: "[QF] Probing Volatility I: Eigen Decomposition & Portfolios Orthogonality"
date: 2020-05-08
tags: [alpha]
header:
  image: "/images/qf_intro_banner.jpg"
excerpt: "Exploring market's volatility through a linear algebraic approach, encompassing topic commonly known as Principal Component Analysis (PCA) in various other non-finance purposes."
mathjax: "true"
---

# The Covariance Matrix
>*Exploring the mathematical applications when tackling volatility & correlational properties of securities in the market, as well as the market itself, by dissecting covariance matrices*


Statistically speaking from an individual approach, for <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;n=1,2,...,N" title="n=1,2,...,N" /> time indexes of <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;r_{i_n}" title="r_{i_n}" /> and  <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;r_{j_n}" title="r_{j_n}" /> as returns data of asset i and j, their covariance <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_{ij}" title="\sigma_{ij}" /> is calculated as:
- <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_{ij}&space;=&space;\frac{\sum_{1}^{N}(r_{i_n}&space;-&space;\bar{r_i})(r_{j_n}&space;-&space;\bar{r_j})}{N-1}" title="\sigma_{ij} = \frac{\sum_{1}^{N}(r_{i_n} - \bar{r_i})(r_{j_n} - \bar{r_j})}{N-1}" /> for **sample size**
- <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_{ij}&space;=&space;\frac{\sum_{1}^{N}(r_{i_n}&space;-&space;\bar{r_i})(r_{j_n}&space;-&space;\bar{r_j})}{N}" title="\sigma_{ij} = \frac{\sum_{1}^{N}(r_{i_n} - \bar{r_i})(r_{j_n} - \bar{r_j})}{N}" /> for **population size**
 
Recall our brief overview in my [initial post](https://jp-quant.github.io/alpha_intro/ "initial post") on the basic essential topics of this research series, given a constructed returns table $$RET$$, we obtain the de-meaned version:
<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\overline{RET}&space;=&space;\begin{vmatrix}&space;(r_{1_1}&space;-&space;\bar{r_{1}})&(r_{2_1}&space;-&space;\bar{r_{2}})&space;&\hdots&space;&(r_{M_1}&space;-&space;\bar{r_{M}})&space;\\&space;(r_{1_2}&space;-&space;\bar{r_{1}})&space;&(r_{2_2}&space;-&space;\bar{r_{2}})&space;&\hdots&space;&(r_{M_2}&space;-&space;\bar{r_{M}})&space;\\&space;\vdots&\vdots&space;&\ddots&space;&\vdots&space;\\&space;(r_{1_N}&space;-&space;\bar{r_{1}})&(r_{2_N}&space;-&space;\bar{r_{2}})&space;&\hdots&space;&(r_{M_N}&space;-&space;\bar{r_{M}})&space;\end{vmatrix}" title="\overline{RET} = \begin{vmatrix} (r_{1_1} - \bar{r_{1}})&(r_{2_1} - \bar{r_{2}}) &\hdots &(r_{M_1} - \bar{r_{M}}) \\ (r_{1_2} - \bar{r_{1}}) &(r_{2_2} - \bar{r_{2}}) &\hdots &(r_{M_2} - \bar{r_{M}}) \\ \vdots&\vdots &\ddots &\vdots \\ (r_{1_N} - \bar{r_{1}})&(r_{2_N} - \bar{r_{2}}) &\hdots &(r_{M_N} - \bar{r_{M}}) \end{vmatrix}" />

A *N x M* matrix representing N interval (hourly/daily/weekly/etc) returns of M securities, we can construct a **sample size** Covariance Matrix (<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}" title="\boldsymbol{C}" />):

<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}&space;=&space;\frac{\overline{RET}^\top&space;\cdot&space;\overline{RET}}{N-1}&space;=&space;\begin{vmatrix}&space;\sigma_1^2&space;&&space;\sigma_{12}&space;&&space;\hdots&space;&&space;\sigma_{1M}&space;\\&space;\sigma_{21}&space;&&space;\sigma_2^2&space;&&space;\hdots&space;&&space;\sigma_{2M}\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;\sigma_{M1}&space;&&space;\sigma_{M2}&space;&\hdots&space;&\sigma_M^2&space;\end{vmatrix}" title="\boldsymbol{C} = \frac{\overline{RET}^\top \cdot \overline{RET}}{N-1} = \begin{vmatrix} \sigma_1^2 & \sigma_{12} & \hdots & \sigma_{1M} \\ \sigma_{21} & \sigma_2^2 & \hdots & \sigma_{2M}\\ \vdots &\vdots &\ddots &\vdots \\ \sigma_{M1} & \sigma_{M2} &\hdots &\sigma_M^2 \end{vmatrix}" />

where <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}" title="\boldsymbol{C}" /> is a *M x M*   **square matrix** such that:
- The diagonal values <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_i^2" title="\sigma_i^2" /> = **variances** of individual securities <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;i&space;=&space;1,2,...,M" title="i = 1,2,...,M" />
- <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_{ij}" title="\sigma_{ij}" /> where <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;i\neq&space;j" title="i\neq j" /> = **covariances** between two different assets i & j

Important points to highlight:
- Although we can find the standard deviation of individual securities <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;i&space;=&space;1,2,...,M" title="i = 1,2,...,M" /> as <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_i&space;=&space;\sqrt{\sigma_i^2}" title="\sigma_i = \sqrt{\sigma_i^2}" /> , there is no "standard deviation" of two securities, where the covariance is written as <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_{ij}" title="\sigma_{ij}" />.
- The covariance value is an **unbounded** measurement value that describes a different perspective, a perspective of looking at the "movement relationship" between two different entities.
- The covariance value <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\sigma_{ij}" title="\sigma_{ij}" /> encompasses/derives a relevant metric **correlation** <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\rho_{ij}&space;=&space;\frac{\sigma_{ij}}{\sigma_i&space;\sigma_j}" title="\rho_{ij} = \frac{\sigma_{ij}}{\sigma_i \sigma_j}" />, where such value, unlike covariance, is **bounded** between (-1,1)

---
For demonstration purposes moving forward, I have preprocessed & cleaned a returns table $$RET$$, logarithmically calculated from **daily close prices from Apil 2019 - April 2020** (encompassing the latest COVID-19 market crash), of 947 securities on the U.S. Equities Market, including individual stocks in all industries/sectors as well as ETFs of domestic (*SPY, XLK, XLV, etc...*) & foreign securities (*FXI, EWJ, etc...* ). Data obtained with IEX Cloud API.

```python
RET
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AMG</th>
      <th>UNH</th>
      <th>EVR</th>
      <th>AMD</th>
      <th>NKE</th>
      <th>NRG</th>
      <th>EV</th>
      <th>VRSN</th>
      <th>SNPS</th>
      <th>PKI</th>
      <th>...</th>
      <th>Z</th>
      <th>PLNT</th>
      <th>PEN</th>
      <th>MSGS</th>
      <th>PSTG</th>
      <th>HPE</th>
      <th>MTCH</th>
      <th>SQ</th>
      <th>TEAM</th>
      <th>UA</th>
    </tr>
    <tr>
      <th>date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4/1/2019</th>
      <td>0.039181</td>
      <td>-0.006981</td>
      <td>0.026032</td>
      <td>0.032385</td>
      <td>0.012040</td>
      <td>-0.016616</td>
      <td>0.023294</td>
      <td>0.023945</td>
      <td>0.020798</td>
      <td>0.012684</td>
      <td>...</td>
      <td>0.016274</td>
      <td>0.014590</td>
      <td>-0.023260</td>
      <td>-0.000205</td>
      <td>0.031175</td>
      <td>0.030005</td>
      <td>-0.006202</td>
      <td>0.018514</td>
      <td>0.013082</td>
      <td>-0.001591</td>
    </tr>
    <tr>
      <th>4/2/2019</th>
      <td>-0.000180</td>
      <td>-0.004613</td>
      <td>-0.001178</td>
      <td>0.014687</td>
      <td>-0.010142</td>
      <td>0.013787</td>
      <td>0.006523</td>
      <td>0.004935</td>
      <td>-0.000510</td>
      <td>-0.001846</td>
      <td>...</td>
      <td>0.024340</td>
      <td>0.012116</td>
      <td>0.005624</td>
      <td>0.014667</td>
      <td>0.013696</td>
      <td>-0.005044</td>
      <td>0.007438</td>
      <td>-0.009479</td>
      <td>0.015079</td>
      <td>0.010034</td>
    </tr>
    <tr>
      <th>4/3/2019</th>
      <td>0.018240</td>
      <td>0.005834</td>
      <td>-0.002468</td>
      <td>0.081451</td>
      <td>0.001185</td>
      <td>-0.000472</td>
      <td>0.006959</td>
      <td>0.006454</td>
      <td>-0.000766</td>
      <td>0.012041</td>
      <td>...</td>
      <td>0.027533</td>
      <td>0.009448</td>
      <td>-0.012190</td>
      <td>0.004362</td>
      <td>0.025559</td>
      <td>0.001895</td>
      <td>0.011228</td>
      <td>0.018998</td>
      <td>-0.002078</td>
      <td>0.005764</td>
    </tr>
    <tr>
      <th>4/4/2019</th>
      <td>0.006065</td>
      <td>0.006285</td>
      <td>0.007814</td>
      <td>0.002409</td>
      <td>0.009544</td>
      <td>-0.002365</td>
      <td>0.010466</td>
      <td>-0.010475</td>
      <td>-0.018567</td>
      <td>-0.007841</td>
      <td>...</td>
      <td>-0.001615</td>
      <td>-0.007042</td>
      <td>-0.016749</td>
      <td>-0.011583</td>
      <td>-0.026876</td>
      <td>0.013785</td>
      <td>-0.037143</td>
      <td>-0.033387</td>
      <td>-0.045579</td>
      <td>0.032385</td>
    </tr>
    <tr>
      <th>4/5/2019</th>
      <td>0.012367</td>
      <td>0.005603</td>
      <td>0.001917</td>
      <td>-0.003789</td>
      <td>0.001406</td>
      <td>-0.004033</td>
      <td>0.006134</td>
      <td>0.016571</td>
      <td>0.011473</td>
      <td>0.011991</td>
      <td>...</td>
      <td>0.003763</td>
      <td>0.010125</td>
      <td>0.004551</td>
      <td>0.006717</td>
      <td>0.004384</td>
      <td>0.004966</td>
      <td>-0.005629</td>
      <td>0.006820</td>
      <td>0.010020</td>
      <td>-0.006597</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3/26/2020</th>
      <td>0.101575</td>
      <td>0.085379</td>
      <td>0.053125</td>
      <td>0.062323</td>
      <td>0.064807</td>
      <td>0.071551</td>
      <td>0.073436</td>
      <td>0.052101</td>
      <td>0.053364</td>
      <td>0.052086</td>
      <td>...</td>
      <td>0.022066</td>
      <td>0.039924</td>
      <td>0.079247</td>
      <td>0.010383</td>
      <td>0.109337</td>
      <td>0.114583</td>
      <td>-0.031277</td>
      <td>0.067172</td>
      <td>0.062938</td>
      <td>0.061196</td>
    </tr>
    <tr>
      <th>3/27/2020</th>
      <td>-0.008481</td>
      <td>-0.051996</td>
      <td>-0.001504</td>
      <td>-0.019558</td>
      <td>-0.012774</td>
      <td>0.045344</td>
      <td>-0.022240</td>
      <td>-0.011378</td>
      <td>-0.028454</td>
      <td>-0.054757</td>
      <td>...</td>
      <td>-0.072186</td>
      <td>-0.038137</td>
      <td>-0.023598</td>
      <td>0.009727</td>
      <td>-0.098243</td>
      <td>-0.069807</td>
      <td>-0.017452</td>
      <td>-0.049201</td>
      <td>-0.040977</td>
      <td>-0.051534</td>
    </tr>
    <tr>
      <th>3/30/2020</th>
      <td>0.069726</td>
      <td>0.035772</td>
      <td>0.003863</td>
      <td>0.027109</td>
      <td>0.025504</td>
      <td>-0.037936</td>
      <td>0.039860</td>
      <td>0.080825</td>
      <td>0.038871</td>
      <td>0.022604</td>
      <td>...</td>
      <td>-0.023468</td>
      <td>-0.036764</td>
      <td>0.049167</td>
      <td>-0.048365</td>
      <td>-0.023118</td>
      <td>0.023152</td>
      <td>0.064909</td>
      <td>0.030647</td>
      <td>-0.007692</td>
      <td>0.010759</td>
    </tr>
    <tr>
      <th>3/31/2020</th>
      <td>-0.021578</td>
      <td>-0.007590</td>
      <td>-0.013585</td>
      <td>-0.051007</td>
      <td>-0.031409</td>
      <td>-0.042728</td>
      <td>-0.046351</td>
      <td>-0.045906</td>
      <td>-0.004029</td>
      <td>-0.016207</td>
      <td>...</td>
      <td>-0.028464</td>
      <td>0.002055</td>
      <td>-0.011524</td>
      <td>-0.064122</td>
      <td>-0.008097</td>
      <td>-0.034416</td>
      <td>-0.009494</td>
      <td>-0.048808</td>
      <td>-0.028017</td>
      <td>-0.042508</td>
    </tr>
    <tr>
      <th>4/1/2020</th>
      <td>-0.051881</td>
      <td>-0.049568</td>
      <td>-0.009599</td>
      <td>-0.040840</td>
      <td>-0.043348</td>
      <td>-0.015527</td>
      <td>-0.062040</td>
      <td>-0.026046</td>
      <td>-0.022219</td>
      <td>-0.060932</td>
      <td>...</td>
      <td>-0.135993</td>
      <td>-0.139947</td>
      <td>-0.049035</td>
      <td>-0.014964</td>
      <td>-0.089231</td>
      <td>-0.013479</td>
      <td>-0.069436</td>
      <td>-0.112428</td>
      <td>-0.023664</td>
      <td>-0.078700</td>
    </tr>
  </tbody>
</table>
</div>

We will use *Pandas* to perform calculation for our covariance matrix, as it is automatically [normalized by (N-1)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cov.html "normalized by (N-1)") for sample population. We confirm this by performing our own calculation to check for sanity

```python
def sample_covariance(ret):
    demeaned_RET = pd.DataFrame({
        s:(ret[s]-ret[s].mean()) for s in ret.columns})
    return ((demeaned_RET.T.dot(demeaned_RET))/(len(ret)-1))

raw_cov = sample_covariance(RET)
pd_cov = RET.cov()
```


```python
False not in [np.allclose(raw_cov[i],pd_cov[i]) for i in raw_cov.columns]
```




    True



They indeed do match. We proceed onto the next step.

## Eigen Decomposition
>*This topic is a part of Linear Algebra, an extremely powerful spectrum within mathematics, that elevated my perspective from reality to abstraction, specifically in modeling nature using infinite-dimensional abstract vector space. For further academic readings for ground-up studies, I recommend watching this MIT lecture [series](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/ "series").*

From a linear algebraic perspective, specifically in our approach, a matrix acts as a vector space, defined by a system of equations, containing subspaces such as row space, column space, etc... as well as an operator that perform transformation, or mapping, of vectors from one subspace to another.
>**Example**: Casting a shadow of an object = mapping a 3D entity to its 2D form

In this work, we are exploring a mapping from N-dimensional space to another N-dimensional space, specifically working with a square matrix. The main point of Eigen Decomposition is simply to find **[invariant subspaces](https://en.wikipedia.org/wiki/Invariant_subspace "invariant subspaces") under transformation by a square matrix**.

Given our *M x M* covariance matrix <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}" title="\boldsymbol{C}" /> of M securities, as square matrix calculated from returns of *N* time indexes, we seek to find <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_i" title="e_i" /> and <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\lambda_i" title="\lambda_i" /> , respectively as the **eigen vectors** and **eigen values**, where for <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;i&space;=&space;1,2,...,M" title="i = 1,2,...,M" />, comes with such **eigen pair** that satisfy the condition:
<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\dpi{150}&space;\boldsymbol{C}&space;\cdot&space;e_i&space;=&space;\lambda_i&space;e_i" title="\boldsymbol{C} \cdot e_i = \lambda_i e_i" /> such that:
- <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_{i}&space;=&space;\begin{vmatrix}&space;e_{i_1}\\&space;e_{i_2}\\&space;\vdots\\&space;e_{i_M}&space;\end{vmatrix}" title="e_{i} = \begin{vmatrix} e_{i_1}\\ e_{i_2}\\ \vdots\\ e_{i_M} \end{vmatrix}" /> , where we can construct an *M x M* matrix of the linearly independent eigen vectors as <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{E}&space;=&space;\begin{vmatrix}&space;e_1&space;&e_2&space;&\hdots&space;&e_M&space;\end{vmatrix}&space;=\begin{vmatrix}&space;e_{1_1}&space;&&space;e_{2_1}&space;&\hdots&space;&e_{M_1}&space;\\&space;e_{1_2}&space;&&space;e_{2_2}&space;&\hdots&space;&e_{M_2}&space;\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;e_{1_M}&space;&e_{2_M}&space;&\hdots&space;&e_{M_M}&space;\end{vmatrix}" title="\boldsymbol{E} = \begin{vmatrix} e_1 &e_2 &\hdots &e_M \end{vmatrix} =\begin{vmatrix} e_{1_1} & e_{2_1} &\hdots &e_{M_1} \\ e_{1_2} & e_{2_2} &\hdots &e_{M_2} \\ \vdots &\vdots &\ddots &\vdots \\ e_{1_M} &e_{2_M} &\hdots &e_{M_M} \end{vmatrix}" />

- <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\lambda_i" title="\lambda_i" /> is a **real** value, each associated with <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_i" title="e_i" />,  to which we can diagonalize it into another *M x M* square matrix:
<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{V}&space;=&space;\begin{vmatrix}&space;\lambda_1&space;&0&space;&\hdots&space;&0&space;\\&space;0&space;&\lambda_2&space;&\hdots&space;&0&space;\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;0&space;&0&space;&\hdots&space;&\lambda_M&space;\end{vmatrix}" title="\boldsymbol{V} = \begin{vmatrix} \lambda_1 &0 &\hdots &0 \\ 0 &\lambda_2 &\hdots &0 \\ \vdots &\vdots &\ddots &\vdots \\ 0 &0 &\hdots &\lambda_M \end{vmatrix}" />


Due to the nature of <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}" title="\boldsymbol{C}" /> being not just as a square matrix, but also a **Hermitian Matrix**, our values will be **real** & the matrix is diagonalizable such that:
<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}&space;=&space;\boldsymbol{E^\top}&space;\cdot&space;\boldsymbol{V}&space;\cdot&space;\boldsymbol{E}&space;=&space;\begin{vmatrix}&space;\sigma_1^2&space;&&space;\sigma_{12}&space;&&space;\hdots&space;&&space;\sigma_{1M}&space;\\&space;\sigma_{21}&space;&&space;\sigma_2^2&space;&&space;\hdots&space;&&space;\sigma_{2M}\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;\sigma_{M1}&space;&&space;\sigma_{M2}&space;&\hdots&space;&\sigma_M^2&space;\end{vmatrix}" title="\boldsymbol{C} = \boldsymbol{E^\top} \cdot \boldsymbol{V} \cdot \boldsymbol{E} = \begin{vmatrix} \sigma_1^2 & \sigma_{12} & \hdots & \sigma_{1M} \\ \sigma_{21} & \sigma_2^2 & \hdots & \sigma_{2M}\\ \vdots &\vdots &\ddots &\vdots \\ \sigma_{M1} & \sigma_{M2} &\hdots &\sigma_M^2 \end{vmatrix}" />

To observe the significance of to why this is powerful when it comes to applying it on the covariance matrix, we will first highlight the significance of the eigen pairs (<img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_i" title="e_i" /> & <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\lambda_i" title="\lambda_i" />)
- Eigen vectors <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_i" title="e_i" /> are **normalized independent vectors** from each other, meaning that they are **orthogonal** to each other with <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;||e_i||&space;=&space;1" title="||e_i|| = 1" />
- Since there are *M* eigen vectors <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_i" title="e_i" /> extracted from an *M x M* matrix, their orthogonality means that they represent the **basis** of M-dimensional vector space.
- Therefore, in other words, we perceive the eigen vectors as the directional vectors basis, the axis that builds the entire subspace that described by such matrix, or in this case the covariance matrix <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\boldsymbol{C}" title="\boldsymbol{C}" />
- Each eigen value <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\lambda_i" title="\lambda_i" /> simply represents the **directional variance** of its associated eigen vector <img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;e_i" title="e_i" /> , independent from one another, such that together they **explain** the total volatility of *M* securities, without any specified allocations.

---

We will first visualize the meaning of such extract eigen pairs by giving an example of two securities -  **SPY**(*S&P500 ETF*) and **IEF**(*7-10 Year Treasury ETF* ).
Plotting their returns against each other in **2-Dimensional Space** and we have:

```python
ax = RET.plot.scatter("SPY","IEF",alpha=0.2)
ax.set_aspect("equal")
```

Observe the relationship between two assets, exhibiting negative correlation, reflecting behaviorial through the financial market (i.e.: As the market goes down, demand falls for equities and rises in treasury, like what happened during COVID-19 market crash). 

```python
RET[["SPY","IEF"]].corr()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SPY</th>
      <th>IEF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>SPY</th>
      <td>1.000000</td>
      <td>-0.516561</td>
    </tr>
    <tr>
      <th>IEF</th>
      <td>-0.516561</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

If we plot the cummulative returns between the two assets, we can see their correlational properties as exhibited through the scatter plot:

```python
(1+RET[["SPY","IEF"]]).cumprod().plot()
```

Using numpy's linalg module to extract our eigen pairs. I have written a function carrying out this calculation with any input $$RET$$ table:

```python
#----| Perform Eigen Decomposition on Covariance Matrix of any given returns table 
#----- dynamic for any NxM size of returns matrix
def PCA(ret):
    eVals,eVecs = np.linalg.eigh(ret.cov())
    eVals = eVals[::-1]
    eVecs = eVecs[::-1]
    _eigens = pd.DataFrame(eVecs.T,columns = eVals,index=ret.columns)
    _eigenNames = ["eigen_"+str(e+1) for e in range(len(_eigens.columns))]
    _eigenVectors = pd.DataFrame(_eigens.values,index=_eigens.index,columns=_eigenNames)
    _eigenValues = pd.Series(list(_eigens.columns),index=_eigenNames,name="eigenValues")
    return _eigenVectors, _eigenValues
```

We can also use scikit to compute this, although to be as specific & transparent as possible, we will stick with our authentic version.

With each eigen vector magnified by 3 times the square root of its associated eigen value (since each eigen value represents the directional variance of such eigen vector, as stated above), graphing such vectors on top of the scatter plot, each in both directions, we obtain:

```python
ax = RET.plot.scatter("SPY","IEF",alpha=0.2)
ret_table = RET[["SPY","IEF"]]

x,y = list(ret_table.columns)
vectors,lambdas = PCA(ret_table)
_start = ret_table.mean(axis=0)
_colors = pd.Series(["red","green"],index=lambdas.index)
for e in lambdas.index:
    v = vectors[e] * (3*np.sqrt(lambdas[e]))
    ax.plot([_start[x]-v[x],_start[x]+v[x]],
              [_start[y]-v[y],_start[y]+v[y]],color=_colors[e],linewidth=2,label=e)
ax.set_aspect("equal")
```

> **REMARK**: The eigen components are orthogonal to each other, encapsulating the magnitudes of standard deviation in both dimensions. We interpret M eigen values as **independently explained variances** that, together, describe the **total variance** of our M selected securities.

We can further perform this step for not just 2-dimensional space (M=2) but also 3-dimensional space (M=3), you can see how such mathematics can take us further to any M-dimensional space such that we can describe the entire variance of M securities , in such abstract vector space that we can't visualize, down to the independent components.

Compiling all the written codes above, adding 3-Dimensional plotting, we compile a **single function** for 2 & 3 dimensional data (to be modified for more functionalities below):


```python
#----| Plot (with labels) & return result of 2 or 3 selected assets with each other with given table
def EIGEN(ret):
    assert(ret.shape[1] in [2,3]),"2-3 assets only"
    fig = plt.figure()
    _ax_ = fig.gca() if (ret.shape[1] == 2) else fig.gca(projection="3d")
    vectors,lambdas = PCA(ret)
    explained_variance = lambdas/lambdas.sum()
    _labels = pd.Series([(i+" - "+str(
                    (explained_variance[i]*100).round(2))+"%") for i in explained_variance.index],
                           index=explained_variance.index)
    result = {"vectors":vectors,"lambdas":lambdas,
              "explained_variance":explained_variance}

    #----| 2-DIMENSIONAL
    if ret.shape[1] == 2:
        x,y = list(ret.columns)
        _ax_.scatter(ret[x], ret[y], alpha=0.1)
        _ax_.set_xlabel(x)
        _ax_.set_ylabel(y)
        _start = ret.mean(axis=0)
        _colors = pd.Series(["red","green"],index=lambdas.index)
        for e in lambdas.index:
            v = vectors[e] * (3*np.sqrt(lambdas[e]))
            _ax_.plot([_start[x]-v[x],_start[x]+v[x]],
                      [_start[y]-v[y],_start[y]+v[y]],
                      color=_colors[e],linewidth=2,label=_labels[e])
        _ax_.set_aspect("equal")
    #---| 3-DIMENSIONAL
    else:
        #----| We can't set_aspect for 3D just yet in matplotlib - build our own custom way of doing it
        def set_axes_equal(ax):
            x_limits = ax.get_xlim3d()
            y_limits = ax.get_ylim3d()
            z_limits = ax.get_zlim3d()

            x_range = abs(x_limits[1] - x_limits[0])
            x_middle = np.mean(x_limits)
            y_range = abs(y_limits[1] - y_limits[0])
            y_middle = np.mean(y_limits)
            z_range = abs(z_limits[1] - z_limits[0])
            z_middle = np.mean(z_limits)
            
            plot_radius = 0.32*max([x_range, y_range, z_range])

            ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])
            ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])
            ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])

        x,y,z = list(ret.columns)
        #-----| Plot returns data
        _ax_.scatter(ret[x], ret[y], ret[z],alpha=0.1)
        _ax_.set_xlabel(x)
        _ax_.set_ylabel(y)
        _ax_.set_zlabel(z)
        _start = ret.mean(axis=0)
        #----| plot Eigen Vectors & magnified with associated Eigen Value
        _colors = pd.Series(["red","green","purple"],index=lambdas.index)
        for e in lambdas.index:
            v = vectors[e] * (3*np.sqrt(lambdas[e]))
            _ax_.quiver(_start[x],_start[y],_start[z],
                       _start[x]+v[x],_start[y]+v[y],_start[z]+v[z],
                       color=_colors[e],lw=2)
            _ax_.quiver(_start[x],_start[y],_start[z],
                       _start[x]-v[x],_start[y]-v[y],_start[z]-v[z],
                       color=_colors[e],lw=2,label=_labels[e])
        set_axes_equal(_ax_)
    _ax_.legend()
    result["axis"] = _ax_
    return result
```

Calling it on our demonstrated example of SHY v.s. IEF, we obtain:

```python
result = EIGEN(RET[["SPY","IEF"]])
```

You can perform this analysis on any such visualizable amount of securities given the returns table inputted. Below are results of various examples I have selected to showcase certain interesting relationships:

[GRAPHS & INFO SHOWCASE IN PROGRESS]





