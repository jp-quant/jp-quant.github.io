---
title: "[QF] Probing Volatility I: Eigen Decomposition & Portfolios Orthogonality"
date: 2020-05-08
tags: [alpha]
header:
  image: "/images/qf_intro_banner.jpg"
excerpt: "Exploring market's volatility utilizing a linear algebraic approach of Eigen Decomposition on Hermitian Matrices, specifically regarded as Principal Component Analysis (PCA) in various other non-finance purposes."
mathjax: "true"
---

# The Covariance Matrix
>*Understanding the mathematical applications when tackling volatility & correlational properties of securities in the market, as well as the market itself.*


Statistically speaking, in a more individual calculation approach, for <img src="https://latex.codecogs.com/gif.latex?n=1,2,...,N" title="n=1,2,...,N" /> time indexes of <img src="https://latex.codecogs.com/gif.latex?r_{i_n}" title="r_{i_n}" /> and  <img src="https://latex.codecogs.com/gif.latex?r_{j_n}" title="r_{j_n}" /> as returns data of asset i and j, their covariance <img src="https://latex.codecogs.com/gif.latex?\sigma_{ij}" title="\sigma_{ij}" /> is calculated as:
- <img src="https://latex.codecogs.com/gif.latex?\sigma_{ij}&space;=&space;\frac{\sum_{1}^{N}(r_{i_n}&space;-&space;\bar{r_i})(r_{j_n}&space;-&space;\bar{r_j})}{N-1}" title="\sigma_{ij} = \frac{\sum_{1}^{N}(r_{i_n} - \bar{r_i})(r_{j_n} - \bar{r_j})}{N-1}" /> for **sample size**
- <img src="https://latex.codecogs.com/gif.latex?\sigma_{ij}&space;=&space;\frac{\sum_{1}^{N}(r_{i_n}&space;-&space;\bar{r_i})(r_{j_n}&space;-&space;\bar{r_j})}{N}" title="\sigma_{ij} = \frac{\sum_{1}^{N}(r_{i_n} - \bar{r_i})(r_{j_n} - \bar{r_j})}{N}" /> for **population size**
 
Recall our brief overview in my [initial post](https://jp-quant.github.io/alpha_intro/ "initial post") on the basic essential topics of this research series, given a constructed table:
<img src="https://latex.codecogs.com/gif.latex?RET&space;=&space;\begin{vmatrix}&space;r_{1_1}&r_{2_1}&space;&\hdots&space;&r_{M_1}&space;\\&space;r_{1_2}&r_{2_2}&space;&\hdots&space;&r_{M_2}&space;\\&space;\vdots&\vdots&space;&\ddots&space;&\vdots&space;\\&space;r_{1_N}&r_{2_N}&space;&\hdots&space;&r_{M_N}&space;\end{vmatrix}" title="RET = \begin{vmatrix} r_{1_1}&r_{2_1} &\hdots &r_{M_1} \\ r_{1_2}&r_{2_2} &\hdots &r_{M_2} \\ \vdots&\vdots &\ddots &\vdots \\ r_{1_N}&r_{2_N} &\hdots &r_{M_N} \end{vmatrix}" /> 

A *N x M* matrix representing N interval (hourly/daily/weekly/etc) returns of M securities, we can construct a  population size Covariance Matrix (<img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}" title="\boldsymbol{C}" />):

<img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}&space;=&space;\frac{RET^\top&space;\cdot&space;RET}{N}&space;=&space;\begin{vmatrix}&space;\sigma_1^2&space;&&space;\sigma_{12}&space;&&space;\hdots&space;&&space;\sigma_{1M}&space;\\&space;\sigma_{21}&space;&&space;\sigma_2^2&space;&&space;\hdots&space;&&space;\sigma_{2M}\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;\sigma_{M1}&space;&&space;\sigma_{M2}&space;&\hdots&space;&\sigma_M^2&space;\end{vmatrix}" title="\boldsymbol{C} = \frac{RET^\top \cdot RET}{N} = \begin{vmatrix} \sigma_1^2 & \sigma_{12} & \hdots & \sigma_{1M} \\ \sigma_{21} & \sigma_2^2 & \hdots & \sigma_{2M}\\ \vdots &\vdots &\ddots &\vdots \\ \sigma_{M1} & \sigma_{M2} &\hdots &\sigma_M^2 \end{vmatrix}" />

where <img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}" title="\boldsymbol{C}" /> is a *M x M*   **square matrix** such that:
- The diagonal values <img src="https://latex.codecogs.com/gif.latex?\sigma_i^2" title="\sigma_i^2" /> = **variances** of individual securities <img src="https://latex.codecogs.com/gif.latex?i&space;=&space;1,2,...,M" title="i = 1,2,...,M" />
- <img src="https://latex.codecogs.com/gif.latex?\sigma_{ij}" title="\sigma_{ij}" /> where <img src="https://latex.codecogs.com/gif.latex?i\neq&space;j" title="i\neq j" /> = **covariances** between two different assets i & j

Important points to highlight:
- Although we can find the standard deviation of individual securities <img src="https://latex.codecogs.com/gif.latex?i&space;=&space;1,2,...,M" title="i = 1,2,...,M" /> as <img src="https://latex.codecogs.com/gif.latex?\sigma_i&space;=&space;\sqrt{\sigma_i^2}" title="\sigma_i = \sqrt{\sigma_i^2}" /> , there is no "standard deviation" of two securities, where the covariance is written as <img src="https://latex.codecogs.com/gif.latex?\sigma_{ij}" title="\sigma_{ij}" />.
- The covariance value is an **unbounded** measurement value that describes a different perspective, a perspective of looking at the "movement relationship" between two different entities.
- The covariance value <img src="https://latex.codecogs.com/gif.latex?\sigma_{ij}" title="\sigma_{ij}" /> encompasses/derives a relevant metric **correlation** <img src="https://latex.codecogs.com/gif.latex?\rho_{ij}&space;=&space;\frac{\sigma_{ij}}{\sigma_i&space;\sigma_j}" title="\rho_{ij} = \frac{\sigma_{ij}}{\sigma_i \sigma_j}" />, where such value, unlike covariance, is **bounded** between (-1,1)


## Eigen Decomposition
>*This topic is a part of Linear Algebra, an extremely powerful spectrum within mathematics, that elevated my perspective from reality to abstraction, specifically in modeling nature using infinite-dimensional abstract vector space. For further academic readings for ground-up studies, I recommend watching this MIT lecture [series](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/ "series").*

From a linear algebraic perspective, specifically in our approach, a matrix acts as a vector space, defined by a system of equations, containing subspaces such as row space, column space, etc... as well as an operator that perform transformation, or mapping, of vectors from one subspace to another.
> **Example**: Casting a shadow of an object = mapping a 3D entity to its 2D form

In this work, we are exploring a mapping from N-dimensional space to another N-dimensional space, specifically working with a square matrix. The main point of Eigen Decomposition is simply to find **[invariant subspaces](https://en.wikipedia.org/wiki/Invariant_subspace "invariant subspaces") under transformation by a square matrix**.

Given our *M x M* covariance matrix <img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}" title="\boldsymbol{C}" /> of M securities, as square matrix calculated from returns of *N* time indexes, we seek to find <img src="https://latex.codecogs.com/gif.latex?e_i" title="e_i" /> and <img src="https://latex.codecogs.com/gif.latex?\lambda_i" title="\lambda_i" /> , respectively as the **eigen vectors** and **eigen values**, where <img src="https://latex.codecogs.com/gif.latex?i&space;=&space;1,2,...,M" title="i = 1,2,...,M" />, such that  <img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}&space;\cdot&space;e_i&space;=&space;\lambda_i&space;e_i" title="\boldsymbol{C} \cdot e_i = \lambda_i e_i" />, where <img src="https://latex.codecogs.com/gif.latex?e_i" title="e_i" /> has the shape of *M x 1* .

Due to the nature of <img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}" title="\boldsymbol{C}" /> being not just as a square matrix, but also a **Hermitian Matrix**, our values will be **real** & the matrix is diagonalizable such that:
<img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}&space;=&space;\begin{vmatrix}&space;e_1&space;&e_2&space;&\hdots&space;&e_M&space;\end{vmatrix}&space;\cdot&space;\begin{vmatrix}&space;\lambda_1&space;&0&space;&\hdots&space;&0&space;\\&space;0&space;&\lambda_2&space;&\hdots&space;&0&space;\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;0&space;&0&space;&\hdots&space;&\lambda_M&space;\end{vmatrix}&space;\cdot&space;\begin{vmatrix}&space;e_1\\&space;e_2\\&space;\vdots\\&space;e_M&space;\end{vmatrix}&space;=&space;\begin{vmatrix}&space;\sigma_1^2&space;&&space;\sigma_{12}&space;&&space;\hdots&space;&&space;\sigma_{1M}&space;\\&space;\sigma_{21}&space;&&space;\sigma_2^2&space;&&space;\hdots&space;&&space;\sigma_{2M}\\&space;\vdots&space;&\vdots&space;&\ddots&space;&\vdots&space;\\&space;\sigma_{M1}&space;&&space;\sigma_{M2}&space;&\hdots&space;&\sigma_M^2&space;\end{vmatrix}" title="\boldsymbol{C} = \begin{vmatrix} e_1 &e_2 &\hdots &e_M \end{vmatrix} \cdot \begin{vmatrix} \lambda_1 &0 &\hdots &0 \\ 0 &\lambda_2 &\hdots &0 \\ \vdots &\vdots &\ddots &\vdots \\ 0 &0 &\hdots &\lambda_M \end{vmatrix} \cdot \begin{vmatrix} e_1\\ e_2\\ \vdots\\ e_M \end{vmatrix} = \begin{vmatrix} \sigma_1^2 & \sigma_{12} & \hdots & \sigma_{1M} \\ \sigma_{21} & \sigma_2^2 & \hdots & \sigma_{2M}\\ \vdots &\vdots &\ddots &\vdots \\ \sigma_{M1} & \sigma_{M2} &\hdots &\sigma_M^2 \end{vmatrix}" />

To observe the significance of to why this is powerful when it comes to applying it on the covariance matrix, we will first highlight the significance of the eigen pairs (<img src="https://latex.codecogs.com/gif.latex?e_i" title="e_i" /> & <img src="https://latex.codecogs.com/gif.latex?\lambda_i" title="\lambda_i" />)
- Eigen vectors <img src="https://latex.codecogs.com/gif.latex?e_i" title="e_i" /> are **normalized independent vectors** from each other, meaning that they are **orthogonal** to each other with <img src="https://latex.codecogs.com/gif.latex?||e_i||&space;=&space;1" title="||e_i|| = 1" />
- Since there are *M* eigen vectors <img src="https://latex.codecogs.com/gif.latex?e_i" title="e_i" /> extracted from an *M x M* matrix, their orthogonality means that they represent the **basis** of M-dimensional vector space.
- Therefore, in other words, we perceive the eigen vectors as the directional vectors basis, the axis that builds the entire subspace that described by such matrix, or in this case the covariance matrix <img src="https://latex.codecogs.com/gif.latex?\boldsymbol{C}" title="\boldsymbol{C}" />
- Each eigen value <img src="https://latex.codecogs.com/gif.latex?\lambda_i" title="\lambda_i" /> simply represents the **directional variance** of its associated eigen vector <img src="https://latex.codecogs.com/gif.latex?e_i" title="e_i" /> , independent from one another, such that together they **explain** the total volatility of *M* securities, without any specified allocations.

We will demonstrate this understanding by visualizing the observable effect with M = 2 & 3 being M-dimensional to be working with.



